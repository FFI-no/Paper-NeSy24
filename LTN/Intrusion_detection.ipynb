{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTN for Knowledge-aware Intrusion Detection\n",
    "This implementation is based on the example implementation from [logictensornetworks](https://github.com/logictensornetworks/logictensornetworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 23:34:59.376156: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"16\"\n",
    "\n",
    "import logging; logging.basicConfig(level=logging.INFO)\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import ltn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define meta variables\n",
    "test_fraction = 0.7\n",
    "training_fraction = 1 - test_fraction\n",
    "batch_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify domain knowledge the IDS2017 dataset description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all ips in the victim network in IDS 2017\n",
    "own_win_servers = [\n",
    "    \"192.168.10.3\",\n",
    "]\n",
    "own_linux_servers = [\n",
    "    \"192.168.10.50\",\n",
    "    \"205.174.165.68\",\n",
    "    \"192.168.10.51\",\n",
    "    \"205.174.165.66\",\n",
    "]\n",
    "\n",
    "own_ubuntu_pcs = [\n",
    "    \"192.168.10.19\",\n",
    "    \"192.168.10.17\",\n",
    "    \"192.168.10.16\",\n",
    "    \"192.168.10.12\",\n",
    "]\n",
    "own_windows_pcs = [\n",
    "    \"192.168.10.9\",\n",
    "    \"192.168.10.5\",\n",
    "    \"192.168.10.8\",\n",
    "    \"192.168.10.14\",\n",
    "    \"192.168.10.15\",\n",
    "]\n",
    "own_mac_pcs = [\n",
    "    \"192.168.10.25\",\n",
    "]\n",
    "\n",
    "own_server = own_win_servers + own_linux_servers\n",
    "own_pcs = own_ubuntu_pcs + own_windows_pcs + own_mac_pcs\n",
    "own_ips = own_server + own_pcs\n",
    "webserver_ips = [\"192.168.10.50\", \"192.168.10.51\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the and partition dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Source IP   Source Port  Destination IP   Destination Port   Protocol  \\\n",
      "0  192.168.10.50         33898    192.168.10.3                389          6   \n",
      "1  192.168.10.50         33904    192.168.10.3                389          6   \n",
      "2        8.6.0.1             0         8.0.6.4                  0          0   \n",
      "\n",
      "    Flow Duration  Total Length of Fwd Packets   Total Length of Bwd Packets  \\\n",
      "0       113095465                         9668                       10012.0   \n",
      "1       113473706                        11364                       12718.0   \n",
      "2       119945515                            0                           0.0   \n",
      "\n",
      "    Fwd Header Length   Bwd Header Length  Fwd PSH Flags  FIN Flag Count  \\\n",
      "0                1536                 768              1               0   \n",
      "1                2176                1280              1               0   \n",
      "2                   0                   0              0               0   \n",
      "\n",
      "    Bwd Packet Length Min  Init_Win_bytes_forward   Init_Win_bytes_backward  \\\n",
      "0                     316                     571                      2079   \n",
      "1                     126                     390                      2081   \n",
      "2                       0                      -1                        -1   \n",
      "\n",
      "    Subflow Fwd Bytes  Total Length of Fwd Packets.1   Label  \n",
      "0                9668                           9668  BENIGN  \n",
      "1               11364                          11364  BENIGN  \n",
      "2                   0                              0  BENIGN  \n"
     ]
    }
   ],
   "source": [
    "with open(\"IDS2017_subset.csv\", \"r\") as file:\n",
    "    dataset = pd.read_csv(file, delimiter=\",\")\n",
    "\n",
    "print(dataset.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all columns between 0 and 1\n",
    "for column in dataset.columns:\n",
    "    if column not in [\" Label\", \" Source IP\", \" Destination IP\"]:\n",
    "        dataset[column] = (dataset[column] - dataset[column].min()) / (\n",
    "            dataset[column].max() - dataset[column].min()\n",
    "        )\n",
    "# Port is a categorical variable, but encoding it as a number works partly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"is_webserver\"] = dataset[\" Destination IP\"].apply(\n",
    "    lambda x: 1 if x in webserver_ips else 0\n",
    ")\n",
    "dataset[\"from_own_network\"] = dataset[\" Source IP\"].apply(\n",
    "    lambda x: 1 if x in own_ips else 0\n",
    ")\n",
    "dataset[\"to_own_network\"] = dataset[\" Destination IP\"].apply(\n",
    "    lambda x: 1 if x in own_ips else 0\n",
    ")\n",
    "## remove the columns \" Source IP\" and \" Destination IP\"\n",
    "dataset = dataset.drop(columns=[\" Source IP\", \" Destination IP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 23:35:01.356867: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# partition the dataset into 4 classes\n",
    "benign = dataset[dataset[\" Label\"] == \"BENIGN\"]\n",
    "brute_force = dataset[dataset[\" Label\"] == \"Web Attack � Brute Force\"]\n",
    "xss = dataset[dataset[\" Label\"] == \"Web Attack � XSS\"]\n",
    "\n",
    "# 70% of each class for training and the remainding 30% for testing\n",
    "benign_train = benign.sample(frac=test_fraction)\n",
    "benign_test = benign.drop(benign_train.index)\n",
    "brute_force_train = brute_force.sample(frac=test_fraction)\n",
    "brute_force_test = brute_force.drop(brute_force_train.index)\n",
    "xss_train = xss.sample(frac=test_fraction)\n",
    "xss_test = xss.drop(xss_train.index)\n",
    "\n",
    "# randomly conatenate all the dataframes\n",
    "training_set = pd.concat([benign_train, brute_force_train, xss_train])\n",
    "test_set = pd.concat([benign_test, brute_force_test, xss_test])\n",
    "# shuffle the dataframes\n",
    "training_set = training_set.sample(frac=1).reset_index(drop=True)\n",
    "test_set = test_set.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "labels_train = training_set.pop(\" Label\")\n",
    "labels_test = test_set.pop(\" Label\")\n",
    "\n",
    "## create a metadata tensor this info is not shown to the neural network\n",
    "metadata_train = training_set[[\"is_webserver\", \"from_own_network\", \"to_own_network\"]]\n",
    "metadata_test = test_set[[\"is_webserver\", \"from_own_network\", \"to_own_network\"]]\n",
    "training_set = training_set.drop(columns=[\"is_webserver\", \"from_own_network\", \"to_own_network\"])\n",
    "test_set = test_set.drop(columns=[\"is_webserver\", \"from_own_network\", \"to_own_network\"])\n",
    "\n",
    "# convert the labels to a numeric value\n",
    "labels_train = labels_train.replace(\"BENIGN\", 0)\n",
    "labels_train = labels_train.replace(\"Web Attack � Brute Force\", 1)\n",
    "labels_train = labels_train.replace(\"Web Attack � XSS\", 2)\n",
    "labels_test = labels_test.replace(\"BENIGN\", 0)\n",
    "labels_test = labels_test.replace(\"Web Attack � Brute Force\", 1)\n",
    "labels_test = labels_test.replace(\"Web Attack � XSS\", 2)\n",
    "\n",
    "# Create batches\n",
    "ds_train = tf.data.Dataset.from_tensor_slices(\n",
    "    (training_set, labels_train, metadata_train)\n",
    ").batch(batch_size)\n",
    "ds_test = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_set, labels_test, metadata_test)\n",
    ").batch(batch_size)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define LTN components\n",
    "Define predicate `P(x,class)`\n",
    "\n",
    "A fully connected MLP (16,16,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    \"\"\"Model that returns logits.\"\"\"\n",
    "\n",
    "    def __init__(self, n_classes, hidden_layer_sizes=(16, 16, 8)):\n",
    "        super(MLP, self).__init__()\n",
    "        self.denses = [\n",
    "            tf.keras.layers.Dense(s, activation=\"elu\") for s in hidden_layer_sizes\n",
    "        ]\n",
    "        self.dense_class = tf.keras.layers.Dense(n_classes)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs[0]\n",
    "        for dense in self.denses:\n",
    "            x = dense(x)\n",
    "            x = self.dropout(x, training=training)\n",
    "        return self.dense_class(x)\n",
    "\n",
    "\n",
    "logits_model = MLP(3)\n",
    "p = ltn.Predicate.FromLogits(\n",
    "    logits_model, activation_function=\"softmax\", with_class_indexing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Claseses `Benign, Brute force, Xss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_benign = ltn.Constant(0, trainable=False)\n",
    "class_brute_force = ltn.Constant(1, trainable=False)\n",
    "class_xss = ltn.Constant(2, trainable=False)\n",
    "\n",
    "def get_class_name(class_index):\n",
    "    if class_index == 0:\n",
    "        return \"BENIGN\"\n",
    "    elif class_index == 1:\n",
    "        return \"Brute Force\"\n",
    "    else:\n",
    "        return \"XSS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=2),semantics=\"forall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Real logic statements \n",
    "\n",
    "<p align=\"center\"> <i> Ground with labeled data </i> </p>\n",
    "\n",
    "$$ \\forall x\\_l\\_benign P(x\\_l\\_benign, Class\\_Benign) $$\n",
    "$$ \\forall x\\_l\\_bruteForce P(x\\_l\\_bruteForce, Class\\_BruteForce) $$\n",
    "$$ \\forall x\\_l\\_xss P(x\\_l\\_xss, Class\\_XSS) $$\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\"> <i>Any non-websever cannot be classified as having a web-attack</i> </p>\n",
    "\n",
    "$$ \\forall x\\_not\\_webserver \\neg (P(x\\_not\\_webserver, Class\\_bruteForce) \\wedge P(x\\_not\\_webserver, Class\\_XSS)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_aggregator = ltn.Wrapper_Formula_Aggregator(\n",
    "    ltn.fuzzy_ops.Aggreg_pMeanError(p=2)\n",
    ")\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def axioms(features, labels, metadata, training=False):     \n",
    "    x_Benign = ltn.Variable(\"x_Benign\", features[labels == 0])\n",
    "    x_Brute_Force = ltn.Variable(\"x_Brute_Force\", features[labels == 1])\n",
    "    x_XSS = ltn.Variable(\"x_XSS\", features[labels == 2])\n",
    "\n",
    "\n",
    "    # traffic from webserver\n",
    "    ws_index = list(metadata_train.columns).index(\"is_webserver\")\n",
    "    x_not_webserver = ltn.Variable(\n",
    "        \"x_not_webserver\", features[metadata[:, ws_index] == 0]\n",
    "    )\n",
    "\n",
    "    axioms = [\n",
    "        Forall(x_Benign, p([x_Benign, class_benign], training=training)),\n",
    "        Forall(x_Brute_Force, p([x_Brute_Force, class_brute_force], training=training)),\n",
    "        Forall(x_XSS, p([x_XSS, class_xss], training=training)),\n",
    "        # only webserver traffic is brute force or xss\n",
    "        Forall(\n",
    "            x_not_webserver,\n",
    "            Not(\n",
    "                Or(\n",
    "                    p([x_not_webserver, class_brute_force], training=training),\n",
    "                    p([x_not_webserver, class_xss], training=training),\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "    sat_level = formula_aggregator(axioms).tensor\n",
    "    return sat_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize all layers and the static graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 23:35:01.510457: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [51104,3]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial sat level 0.31719\n"
     ]
    }
   ],
   "source": [
    "for features, labels, metadata in ds_test:\n",
    "    print(\"Initial sat level %.5f\"%axioms(features, labels, metadata))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "    'train_sat_kb': tf.keras.metrics.Mean(name='train_sat_kb'),\n",
    "    'test_sat_kb': tf.keras.metrics.Mean(name='test_sat_kb'),\n",
    "    'train_accuracy': tf.keras.metrics.CategoricalAccuracy(name=\"train_accuracy\"),\n",
    "    'test_accuracy': tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "@tf.function\n",
    "def train_step(features, labels, metadata):\n",
    "    # sat and update\n",
    "    with tf.GradientTape() as tape:\n",
    "        sat = axioms(features, labels, metadata, training=True)\n",
    "        loss = 1.-sat\n",
    "    gradients = tape.gradient(loss, p.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, p.trainable_variables))\n",
    "\n",
    "    sat = axioms(features, labels, metadata) # compute sat without dropout\n",
    "    metrics_dict['train_sat_kb'](sat)\n",
    "    # accuracy\n",
    "    predictions = logits_model([features])\n",
    "    metrics_dict['train_accuracy'](tf.one_hot(labels,3),predictions)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(features, labels, metadata):\n",
    "    # sat\n",
    "    sat = axioms(features, labels, metadata)\n",
    "    metrics_dict['test_sat_kb'](sat)\n",
    "    # accuracy\n",
    "    predictions = logits_model([features])\n",
    "    metrics_dict['test_accuracy'](tf.one_hot(labels,3),predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 23:35:05.707801: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [119240,3]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "2024-04-27 23:35:06.321130: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'features' with dtype double and shape [10000,15]\n",
      "\t [[{{node features}}]]\n",
      "2024-04-27 23:35:07.016683: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'features' with dtype double and shape [10000,15]\n",
      "\t [[{{node features}}]]\n",
      "2024-04-27 23:35:07.076962: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'features' with dtype double and shape [10000,15]\n",
      "\t [[{{node features}}]]\n",
      "2024-04-27 23:35:07.198521: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'features' with dtype double and shape [10000,15]\n",
      "\t [[{{node features}}]]\n",
      "2024-04-27 23:35:11.990495: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'features' with dtype double and shape [9240,15]\n",
      "\t [[{{node features}}]]\n",
      "2024-04-27 23:35:12.933172: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'features' with dtype double and shape [9240,15]\n",
      "\t [[{{node features}}]]\n",
      "2024-04-27 23:35:14.042054: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'features' with dtype double and shape [10000,15]\n",
      "\t [[{{node features}}]]\n",
      "2024-04-27 23:35:14.928838: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'features' with dtype double and shape [1104,15]\n",
      "\t [[{{node features}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_sat_kb: 0.3372, test_sat_kb: 0.3516, train_accuracy: 0.1014, test_accuracy: 0.1931\n",
      "Epoch 20, train_sat_kb: 0.5736, test_sat_kb: 0.5549, train_accuracy: 0.9185, test_accuracy: 0.9223\n",
      "Epoch 40, train_sat_kb: 0.5824, test_sat_kb: 0.5607, train_accuracy: 0.9296, test_accuracy: 0.9276\n",
      "Epoch 60, train_sat_kb: 0.5892, test_sat_kb: 0.5667, train_accuracy: 0.9489, test_accuracy: 0.9471\n",
      "Epoch 80, train_sat_kb: 0.5916, test_sat_kb: 0.5684, train_accuracy: 0.9510, test_accuracy: 0.9505\n",
      "Epoch 100, train_sat_kb: 0.5934, test_sat_kb: 0.5692, train_accuracy: 0.9524, test_accuracy: 0.9509\n",
      "Epoch 120, train_sat_kb: 0.5953, test_sat_kb: 0.5708, train_accuracy: 0.9525, test_accuracy: 0.9518\n",
      "Epoch 140, train_sat_kb: 0.5973, test_sat_kb: 0.5728, train_accuracy: 0.9538, test_accuracy: 0.9506\n",
      "Epoch 160, train_sat_kb: 0.5997, test_sat_kb: 0.5742, train_accuracy: 0.9541, test_accuracy: 0.9542\n",
      "Epoch 180, train_sat_kb: 0.6026, test_sat_kb: 0.5769, train_accuracy: 0.9561, test_accuracy: 0.9547\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import commons\n",
    "importlib.reload(commons)\n",
    "EPOCHS = 200\n",
    " \n",
    "commons.train(\n",
    "    EPOCHS,\n",
    "    metrics_dict,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    train_step,\n",
    "    test_step,\n",
    "    csv_path=\"training_results.csv\",\n",
    "    track_metrics=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    Label    |  TP    |   FP  |   FN  | Precision | Recall |   F1  |  MCC  |\n",
      "|-------------|--------|-------|-------|-----------|--------|-------|-------|\n",
      "| BENIGN      |  48341 |    85 |  2115 |     0.998 |  0.958 | 0.978 | 0.415 |\n",
      "| Brute Force |    287 |  1830 |   165 |     0.136 |  0.635 | 0.223 | 0.281 |\n",
      "| XSS         |    126 |   435 |    70 |     0.225 |  0.643 | 0.333 | 0.376 |\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "predictions = logits_model([tf.convert_to_tensor(test_set)])\n",
    "confusion_matrix = tf.math.confusion_matrix(\n",
    "    labels_test, tf.argmax(predictions, axis=1), num_classes=3\n",
    ")\n",
    "\n",
    "\n",
    "print(\"|    Label    |  TP    |   FP  |   FN  | Precision | Recall |   F1  |  MCC  |\")\n",
    "print(\"|-------------|--------|-------|-------|-----------|--------|-------|-------|\")\n",
    "\n",
    "# for each class, compute the precision, recall and f1 score\n",
    "for i in range(3):\n",
    "    tp = int((confusion_matrix[i, i]))\n",
    "    fn = int(tf.reduce_sum(confusion_matrix[i, :]) - confusion_matrix[i, i])\n",
    "    fp = int(tf.reduce_sum(confusion_matrix[:, i]) - confusion_matrix[i, i])\n",
    "    tn = int(tf.reduce_sum(confusion_matrix) - tp - fp - fn)\n",
    "\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    mcc = (tp * tn - fp * fn) / math.sqrt(((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)))\n",
    "    print(\n",
    "        f\"| {get_class_name(i):11} | {tp:6} | {fp:5} | {fn:5} | {precision:9.3f} | {recall:6.3f} | {f1:5.3f} | {mcc:3.3f} |\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
